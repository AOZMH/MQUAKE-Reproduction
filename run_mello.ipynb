{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd6e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../MQuAKE/')\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84644ce",
   "metadata": {},
   "source": [
    "#### Set up OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1588cf93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_141759/3841488683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def call_gpt(cur_prompt, stop):\n",
    "    ans = openai.Completion.create(\n",
    "                model=\"text-davinci-003\",\n",
    "                max_tokens=256,\n",
    "                stop=stop,\n",
    "                prompt=cur_prompt,\n",
    "                temperature=0)\n",
    "    returned = ans['choices'][0]['text']\n",
    "    return returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc4bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangminhao/miniconda3/envs/asdf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "gptj_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", device_map='auto')\n",
    "gptj_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58b5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gptj_model.cuda()\n",
    "# for nm, par in gptj_model.named_parameters():\n",
    "#     print(nm, par.shape, par.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562b4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria\n",
    "\n",
    "class StopByWords(StoppingCriteria):\n",
    "    # Stopping-words criteria\n",
    "    def __init__(self, stop_words, tokenizer):\n",
    "        StoppingCriteria.__init__(self)\n",
    "        # NOTE stop words应在stirng层面而非token ids层面判断\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        # self.stop_word_ids = []\n",
    "        # for sw in set(stop_words):\n",
    "        #     sw_ids = tokenizer.encode(sw)\n",
    "        #     self.stop_word_ids.append(sw_ids)\n",
    "\n",
    "    def __call__(self, input_ids, scores=None):\n",
    "        # NOTE 提高decoding性能, 限制suffix在最后10个token内\n",
    "        suffix_str = self.tokenizer.decode(input_ids[0][-10:]).strip()\n",
    "        for sw in self.stop_words:\n",
    "            if suffix_str.endswith(sw):\n",
    "                return True\n",
    "        return False\n",
    "        # for sw_ids in self.stop_word_ids:\n",
    "        #     if input_ids[0][-len(sw_ids):].tolist() == sw_ids:\n",
    "        #         return True\n",
    "\n",
    "class gptj_interface:\n",
    "    # Wrapper for GPT-J generation, including customization of the generation config & stopping-words\n",
    "    def __init__(self, gptj_tokenizer, gptj_model, stop_words, gen_config={}):\n",
    "        self.tokenizer = gptj_tokenizer\n",
    "        self.model = gptj_model\n",
    "        self.cfg = gen_config\n",
    "        self.stopper = StopByWords(stop_words, gptj_tokenizer)\n",
    "\n",
    "    def call_gptj_local(self, cur_prompt):\n",
    "        # <1, prompt_len>\n",
    "        inputs_dict = self.tokenizer(cur_prompt, return_tensors='pt').to(self.model.device)\n",
    "        output_dict = self.model.generate(**inputs_dict,\n",
    "                                        return_dict_in_generate=True,\n",
    "                                        #output_attentions=True,\n",
    "                                        repetition_penalty = self.cfg.get('repetition_penalty', 0.),\n",
    "                                        temperature = self.cfg.get('temperature', 1.),\n",
    "                                        top_k = self.cfg.get('top_k', 1),\n",
    "                                        top_p = self.cfg.get('top_p', 1.),\n",
    "                                        max_new_tokens = self.cfg.get('max_new_tokens', 32),\n",
    "                                        do_sample = self.cfg.get('do_sample', True),\n",
    "                                        pad_token_id = 50256,\n",
    "                                        stopping_criteria = [self.stopper,],\n",
    "                                        num_return_sequences = 5,)\n",
    "        topk_output_ids = output_dict.sequences\n",
    "        #print(output_dict.keys(), topk_output_ids.shape)\n",
    "        output_sents = []\n",
    "        for output_ids in topk_output_ids:\n",
    "            output_sent = self.tokenizer.decode(output_ids)\n",
    "            output_sents.append(output_sent)\n",
    "        return output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "915d042c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.gptj_interface at 0x7fe3c91364c0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue_config = {\n",
    "    'repetition_penalty' : 1.05,\n",
    "    'temperature' : 0.3,\n",
    "    'top_k' : 5,\n",
    "    'top_p' : 0.85,\n",
    "    'max_new_tokens' : 32,\n",
    "    'do_sample' : True,\n",
    "}\n",
    "\n",
    "tmp_stoppers = ['.']\n",
    "\n",
    "gptj_api = gptj_interface(gptj_tokenizer, gptj_model, tmp_stoppers, clue_config)\n",
    "gptj_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ae6b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ellie Kemper is a citizen of the world.\n",
      "\n",
      "The actress, who plays Carol on The Office, has lived in London, Paris, New York and Los Angeles, and she’s currently living in Los Angeles with her husband, actor Paul F. Tompkins. But when it comes to her career, Kemper has never been afraid \n",
      "\n",
      "Ellie Kemper is a citizen of the world. She’s lived in London, Paris and New York City. She’s traveled to more than 40 countries on five continents. She’s been to the top of Mount Kilimanjaro, the bottom of the Grand Canyon, and the middle of the Sahara Desert.\n",
      "\n",
      "She \n",
      "\n",
      "Ellie Kemper is a citizen of the world.\n",
      "\n",
      "The actress, best known for her role as Annie Edison on The Office, has lived in London, Los Angeles and New York City, and she’s currently based in Austin, Texas. But she’s never been to Paris.\n",
      "\n",
      "Until now.\n",
      "\n",
      "Kemper \n",
      "\n",
      "Ellie Kemper is a citizen of the world.\n",
      "\n",
      "The actress, who plays Carol on The Office, has been to more than 60 countries and has lived in New York, Los Angeles, London, Paris, and Berlin. She’s also spent time living in her native Ohio.\n",
      "\n",
      "“I’ve been to all 50 \n",
      "\n",
      "Ellie Kemper is a citizen of the world.\n",
      "\n",
      "The actress, who plays the titular character on The Office, has lived in London, New York and Los Angeles, and she’s currently living in Paris with her husband, actor Chris Messina.\n",
      "\n",
      "She’s also a mother to two sons, ages 3 and 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_sents = gptj_api.call_gptj_local('Ellie Kemper is a citizen of')\n",
    "for sent in output_sents:\n",
    "    print(sent, '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d03c32bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ellie Kemper is a citizen of the world.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36840529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æĹ', '»', 'æĹ', '»', 'æĹ', '»']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gptj_tokenizer.tokenize('旻旻旻')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba50f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "964a79bd",
   "metadata": {},
   "source": [
    "#### Functions for retrieval models (Contriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd5d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "\n",
    "def get_sent_embeddings(sents, contriever, tok, BSZ=32):    \n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(sents), BSZ)):\n",
    "        sent_batch = sents[i:i+BSZ]\n",
    "        inputs = tok(sent_batch, padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = contriever(**inputs)\n",
    "            embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "        all_embs.append(embeddings.cpu())\n",
    "    all_embs = torch.vstack(all_embs)\n",
    "    return all_embs\n",
    "\n",
    "def retrieve_facts(query, fact_embs, contriever, tok, k=1):\n",
    "    inputs = tok([query], padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = contriever(**inputs)\n",
    "        query_emb = mean_pooling(outputs[0], inputs['attention_mask']).cpu()\n",
    "    sim = (query_emb @ fact_embs.T)[0]\n",
    "    knn = sim.topk(k, largest=True)\n",
    "    return knn.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771712cb",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469cccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/MQuAKE-CF-3k.json', 'r') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded19987",
   "metadata": {},
   "source": [
    "#### Build a memory index which contains all the edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f098daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_facts = set()\n",
    "for d in dataset:\n",
    "    for r in d[\"requested_rewrite\"]:\n",
    "        new_facts.add(f'{r[\"prompt\"].format(r[\"subject\"])} {r[\"target_new\"][\"str\"]}')\n",
    "new_facts = list(new_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a060c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contriever = AutoModel.from_pretrained(\"facebook/contriever-msmarco\").cuda()\n",
    "# contriever = AutoModel.from_pretrained(\"facebook/contriever-msmarco\", device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4eaa2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 88/88 [00:02<00:00, 36.48it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = get_sent_embeddings(new_facts, contriever, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d605e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the current head of state in United States of America is Norodom Sihamoni\n"
     ]
    }
   ],
   "source": [
    "# Run test for retrieval index\n",
    "fact_ids = retrieve_facts(\"Who is the president of the US?\", embs, contriever, tokenizer)\n",
    "print(new_facts[fact_ids[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0937cc8",
   "metadata": {},
   "source": [
    "#### Run MeLLo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b75fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prompts\n",
    "with open('prompts/MeLLo-prompt.txt', 'r') as f:\n",
    "    task_prompt = f.read()\n",
    "mquake_stop = [\"Retrieved fact:\", \"Question:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9b2e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.gptj_interface at 0x7f1592c94790>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue_config = {\n",
    "    'repetition_penalty' : 1.05,\n",
    "    'temperature' : 0.3,\n",
    "    'top_k' : 5,\n",
    "    'top_p' : 0.85,\n",
    "    'max_new_tokens' : 64,\n",
    "    'do_sample' : True,\n",
    "}\n",
    "\n",
    "gptj_api = gptj_interface(gptj_tokenizer, gptj_model, mquake_stop, clue_config)\n",
    "gptj_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecbf77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8c17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edd2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffdefa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████▋                  | 919/1436 [72:57:12<40:47:27, 284.04s/it]"
     ]
    }
   ],
   "source": [
    "# Run MeLLo on the first T (T=10) examples\n",
    "T = 10\n",
    "\n",
    "#cor = 0\n",
    "#tot = 0\n",
    "cor, tot = 339, 2829\n",
    "\n",
    "fout = open('trial.log', 'a')\n",
    "\n",
    "for d in tqdm(dataset[tot:]):\n",
    "    tot += 1\n",
    "    for q in d[\"questions\"]:\n",
    "        found_ans = False\n",
    "        prompt = task_prompt + \"\\n\\nQustion: \" + q\n",
    "        print('======================================\\n[Question]', q, file=fout)\n",
    "        for i in range(4):\n",
    "            # prompt the model to generate a subquestion and a tentative answer\n",
    "            #gen = call_gpt(prompt, mquake_stop)\n",
    "            llm_output = gptj_api.call_gptj_local(prompt)\n",
    "            # 直接选择top-1\n",
    "            gen = llm_output[0]\n",
    "            print('\\n--------~~~~~~~~--------\\n', gen[len(task_prompt)+2 : ], end='\\n------------------------\\n', file=fout, flush=True)\n",
    "            \n",
    "            # if final answer is there, get the answer and exit\n",
    "            # NOTE GPTJ不会结束生成, 因此将下一个Question的生成也作为finalize触发条件\n",
    "            last_sent, prev_sent = gen.strip().split('\\n')[-1], gen.strip().split('\\n')[-3]\n",
    "            if last_sent.startswith('Final answer: '):\n",
    "                ans = last_sent[len(\"Final answer: \"):]\n",
    "                found_ans = True\n",
    "            if last_sent.startswith('Question:'):\n",
    "                assert(prev_sent.startswith('Final answer: '))\n",
    "                ans = prev_sent[len(\"Final answer: \"):]\n",
    "                found_ans = True\n",
    "            if found_ans:\n",
    "                print('[Found Answer]', ans, file=fout)\n",
    "                break\n",
    "            \n",
    "            # otherwise, extract the generated subquestion\n",
    "            if len(gen.strip().split('\\n')) < 2:\n",
    "                print('[Generation Error] Only one line', file=fout)\n",
    "                break # failed case\n",
    "\n",
    "            # NOTE StoppingCriteria会保留stop words, 此处更新逻辑以跳过最后Retrieved fact行\n",
    "            subquestion = gen.strip().split('\\n')[-3]\n",
    "            if not subquestion.startswith('Subquestion: '):\n",
    "                print('[Subquestion Prefix Error]', subquestion, file=fout)\n",
    "                break # failed case\n",
    "            subquestion = subquestion[len(\"Subquestion: \"):]\n",
    "            \n",
    "            # retrieve an edited fact using the generated subquestion\n",
    "            fact_ids = retrieve_facts(subquestion, embs, contriever, tokenizer)\n",
    "            fact_sent = new_facts[fact_ids[0]]\n",
    "            \n",
    "            # put the retrieved fact at the end of the prompt, the model self-checks if it contradicts\n",
    "            #prompt = prompt + gen + 'Retrieved fact: ' + fact_sent + '.'\n",
    "            # NOTE transformers的generate结果会保留input, 此处fix prompt更新逻辑\n",
    "            # 此外, 也移除额外生成的retrieved fact\n",
    "            prompt = gen.strip()[:-len('\\nRetrieved fact:')]\n",
    "            prompt += '\\nRetrieved fact: ' + fact_sent + '.'\n",
    "                    \n",
    "        if not found_ans:\n",
    "            continue\n",
    "        # if the answer is correct\n",
    "        if ans == d[\"new_answer\"] or ans in d[\"new_answer_alias\"]:\n",
    "            cor += 1\n",
    "            break\n",
    "    \n",
    "    print('Running acc = {} / {} = {}'.format(cor, tot, cor/tot), file=fout)\n",
    "\n",
    "print(f'Multi-hop acc = {cor / tot} ({cor} / {tot})', file=fout)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819db10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2E unlearning\n",
    "e2e_prompt = d['questions'][0]\n",
    "\n",
    "# Ad-hoc unlearning\n",
    "edit_statements = []\n",
    "edit_facts = d['requested_rewrite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a74c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339, 2830)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor, tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e28f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1d4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4645c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 21:57:55,573\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#os.chdir('../MQuAKE/')\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import ray\n",
    "ray.init(num_cpus=24)\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbea645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 21:57:59,901\tINFO worker.py:1585 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-14 21:58:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='EleutherAI/gpt-j-6B', tokenizer='EleutherAI/gpt-j-6B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-14 21:58:07 selector.py:16] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:08 selector.py:16] Using FlashAttention backend.\n",
      "INFO 04-14 21:58:08 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:08 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "INFO 04-14 21:58:12 weight_utils.py:177] Using model weights format ['*.bin']\n",
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:12 weight_utils.py:177] Using model weights format ['*.bin']\n",
      "INFO 04-14 21:58:31 model_runner.py:104] Loading model weights took 5.6381 GB\n",
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:34 model_runner.py:104] Loading model weights took 5.6381 GB\n",
      "INFO 04-14 21:58:35 ray_gpu_executor.py:240] # GPU blocks: 8532, # CPU blocks: 1170\n",
      "INFO 04-14 21:58:39 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-14 21:58:39 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:39 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:39 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-14 21:58:44 model_runner.py:867] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model = \"EleutherAI/gpt-j-6B\",\n",
    "    tokenizer = \"EleutherAI/gpt-j-6B\",\n",
    "    # dtype = 'float32',\n",
    "    # Use all GPUs\n",
    "    tensor_parallel_size = torch.cuda.device_count(),\n",
    "    # tensor_parallel_size = 1,\n",
    "    gpu_memory_utilization = 0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7e0f3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_config = SamplingParams(\n",
    "    # repetition_penalty = gen_config.get('repetition_penalty', 0.),\n",
    "    temperature = 0.,\n",
    "    best_of = 3,\n",
    "    # n = gen_config.get('top_k', 1),\n",
    "    # top_k = gen_config.get('top_k', 1),\n",
    "    # top_p = gen_config.get('top_p', 1.),\n",
    "    max_tokens = 64,\n",
    "    min_tokens = 24,\n",
    "    stop = [\"Retrieved fact:\", \"\\n\\n\"],\n",
    "    include_stop_str_in_output = True,\n",
    "    use_beam_search = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547068a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=231378)\u001b[0m INFO 04-14 21:58:44 model_runner.py:867] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "sampling_config = SamplingParams(\n",
    "    repetition_penalty = 1.05,\n",
    "    temperature = 0.3,\n",
    "    top_k = 5,\n",
    "    top_p = 0.85,\n",
    "    max_tokens = 64,\n",
    "    use_beam_search = False,\n",
    "    stop = [\"Retrieved fact:\", \"\\n\\n\"],\n",
    "    include_stop_str_in_output = True,\n",
    "    best_of = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a599e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "lo = llm.generate('Question: Who\\'s the US president?', sampling_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3d994",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0835f8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CompletionOutput(index=1, text=' A. The US President\\nB- The President of the USA C-The US President (Presidential)\\nC+The president is a political officer of USA, which holds a position as an officer in the executive department. D+He has a position in the cabinet, as the highest executive official, who is', token_ids=[317, 13, 383, 1294, 1992, 198, 33, 12, 383, 1992, 286, 262, 4916, 327, 12, 464, 1294, 1992, 357, 10364, 498, 8, 198, 34, 10, 464, 1893, 318, 257, 1964, 3818, 286, 4916, 11, 543, 6622, 257, 2292, 355, 281, 3818, 287, 262, 4640, 5011, 13, 360, 10, 1544, 468, 257, 2292, 287, 262, 13447, 11, 355, 262, 4511, 4640, 1743, 11, 508, 318], cumulative_logprob=-103.1183865070343, logprobs=None, finish_reason=length, stop_reason=None),\n",
       " CompletionOutput(index=0, text='\\nAnswer 1: Barack ObamanObama, a/l. (B) (Barac, Barack), né Barack\\nObama Sr., born 1961, a.l./n.l. of a.l.. His wife, a./o\\nSuey-Chi Obama, né Dunham', token_ids=[198, 33706, 352, 25, 8732, 1835, 10546, 15948, 11, 257, 14, 75, 13, 357, 33, 8, 357, 10374, 330, 11, 2409, 441, 828, 299, 2634, 8732, 198, 15948, 21714, 1539, 4642, 20510, 11, 257, 13, 75, 19571, 77, 13, 75, 13, 286, 257, 13, 75, 492, 2399, 3656, 11, 257, 19571, 78, 198, 50, 518, 88, 12, 1925, 72, 2486, 11, 299, 2634, 44588], cumulative_logprob=-103.50768506526947, logprobs=None, finish_reason=length, stop_reason=None),\n",
       " CompletionOutput(index=2, text='\\nA question for you: If I asked a random group on campus if they\\nknow what country they are currently a resident or inhabit\\nand then I told them that this country had no leader but was\\ninstead a republic and the people of said state voted on\\nwhether their representatives in congress were approved\\nby a', token_ids=[198, 32, 1808, 329, 345, 25, 1002, 314, 1965, 257, 4738, 1448, 319, 7611, 611, 484, 198, 16275, 644, 1499, 484, 389, 3058, 257, 6623, 393, 14527, 198, 392, 788, 314, 1297, 606, 326, 428, 1499, 550, 645, 3554, 475, 373, 198, 38070, 257, 17146, 290, 262, 661, 286, 531, 1181, 7052, 319, 198, 25356, 511, 10826, 287, 8681, 547, 6325, 198, 1525, 257], cumulative_logprob=-103.64666533470154, logprobs=None, finish_reason=length, stop_reason=None)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44088d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082254bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d9fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
